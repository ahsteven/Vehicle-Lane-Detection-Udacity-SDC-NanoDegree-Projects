{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane and Vehicle Detection\n",
    "This repo contains the code for two projects in the Udacity Self-Driving Car NanoDegree: Lane Detection and Vehicle Detection.\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/KISHFmMYJnc/0.jpg)](https://www.youtube.com/watch?v=KISHFmMYJnc)\n",
    "\n",
    "### Lane Detection\n",
    "The steps for lane detection are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/kGVAMj7J2LI/0.jpg)](https://www.youtube.com/watch?v=kGVAMj7J2LI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods used to create create a thresholded binary image containing likely lane pixels.\n",
    "The parameters used were in each method were tuned first seperately and then fine tuned using the combination of methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3dfa5e61c8ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[1;31m# gradients outside of an expected range of line gradients are thresholded out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[1;32mdef\u001b[0m \u001b[0mdir_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msobel_kernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[1;31m# Calculate the x and y gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# This method takes an image and the desired gradent orientation\n",
    "# and comutes a sobel edge detected image in that gradient direction.\n",
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# cv2\n",
    "    \n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0,ksize=sobel_kernel))\n",
    "\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1,ksize=sobel_kernel))\n",
    "        \n",
    "    \n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "\n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    # Here I'm using inclusive (>=, <=) thresholds, but exclusive is ok too\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1  \n",
    "\n",
    "    return binary_output\n",
    "\n",
    "# This method calculates the sobel in both x and y \n",
    "# and filteres out gradients below a certain magnitude\n",
    "def mag_thresh(image, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # Calculate gradient magnitude\n",
    "    # Apply threshold\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)#cv2\n",
    "    \n",
    "    # Take both Sobel x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8) \n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "# gradients outside of an expected range of line gradients are thresholded out\n",
    "def dir_threshold(image, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate the x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Take the absolute value of the gradient direction, \n",
    "    # apply a threshold, and create a binary image result\n",
    "    absgraddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "    binary_output =  np.zeros_like(absgraddir)\n",
    "    binary_output[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "# a hugh light saturation image s chanel is used to pick up lane markings\n",
    "def hls_threshold(image, s_thresh=(170, 255)):   \n",
    "    hls = cv2.cvtColor(image, cv2.COLOR_BGR2HLS).astype(np.float)\n",
    "    s_channel = hls[:,:,2]\n",
    "    # Threshold saturation channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1    \n",
    "    return s_binary\n",
    "\n",
    "# the red channel on the RGB spectrum was seen to be effetive in distinquishing the lane markings\n",
    "def red_threshold(image, r_thresh=(170, 255)):   \n",
    "\n",
    "    r_channel = image[:,:,2] # using cv2 to read\n",
    "    # Threshold saturation channel\n",
    "    r_binary = np.zeros_like(r_channel)\n",
    "    r_binary[(r_channel >= r_thresh[0]) & (r_channel <= r_thresh[1])] = 1    \n",
    "    return r_binary\n",
    "\n",
    "def pipeline(image):\n",
    "\n",
    "    \n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 15 # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(image, orient='x', sobel_kernel=ksize, thresh=(100, 255))\n",
    "    grady = abs_sobel_thresh(image, orient='y', sobel_kernel=ksize, thresh=(130, 255))\n",
    "    mag_binary = mag_thresh(image, sobel_kernel=ksize, mag_thresh=(70, 255))\n",
    "    dir_binary = dir_threshold(image, sobel_kernel=ksize, thresh=(np.pi/2-1, np.pi/2-.35))\n",
    "    s_binary = hls_threshold(image, s_thresh=(150, 255))\n",
    "    red_binary = red_threshold(image, r_thresh=(200, 255))\n",
    "    combined = np.zeros_like(gradx)\n",
    "    combined[((gradx == 1) | (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))|((red_binary == 1)&(s_binary == 1))] = 1\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](test_images/laneEdges.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perspective Transform\n",
    "\n",
    "The perspective transform gives a birds eye view of the lane which allows us to calculate the radius of curvature of the lane. The parameters were tuned using an image of a fairly straight stretch of road. The four source points were determined by placing marks on the lane at a location at the bottom of the image and further down the road. They were fine tuned by observing how straight the lines appeared after the transform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This method returns the perspective transform matrix and its inverse\n",
    "def get_M_Minv(img):\n",
    "    img_size = (img.shape[1],img.shape[0])  \n",
    "    src = np.float32(\n",
    "    [[578, 460],\n",
    "    [200, 720],\n",
    "    [1090, 720],\n",
    "    [692, 460]])  \n",
    "    dst = np.float32(\n",
    "    [[320, 0],\n",
    "    [320, 720],\n",
    "    [960, 720],\n",
    "    [960, 0]])\n",
    "    # compute perspective transform\n",
    "    M = cv2.getPerspectiveTransform(src,dst)    \n",
    "    Minv = cv2.getPerspectiveTransform(dst,src)   \n",
    "    return M, Minv\n",
    "\n",
    "# gets perspective transformed image\n",
    "def warp(img, M):\n",
    "    img_size = (img.shape[1],img.shape[0])   \n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    return warped\n",
    "\n",
    "# unwarps the image back to original\n",
    "def unWarp(img, Minv):\n",
    "    img_size = (img.shape[1],img.shape[0])    \n",
    "    warped = cv2.warpPerspective(img, Minv, img_size, flags=cv2.INTER_LINEAR)\n",
    "    return unWarped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below plots the points used in the original source image and displays the respective perspective transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![](test_images/perspectiveTransform.png)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the below function takes in a set of vertices and masks out all pixels outside of this polygon\n",
    "# The mask region was made slightly larger than the lane itself to insure sharper curves were also included\n",
    "def maskRegion(image, vertices):\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(image) \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(image.shape) > 2:\n",
    "        channel_count = image.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255 \n",
    "\n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(image, mask)\n",
    "    \n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask out area not in region of interest, perform perspective transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](test_images/Masked_lane.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search image for Pixels for polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This methods determines where the base of the lane is using a histogram\n",
    "def getHist(warped):\n",
    "    histogram = np.sum(warped[int(warped.shape[0]/2):,:], axis=0)\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extractForFit function calculates the base location of the left and right line perspective pixels using a histogram. \n",
    "Then if the previous line was not detected. A window is slid across the warped image around some margin window \n",
    "capturing the points within that window. If the number of points is significant the window is recentered around the mean of \n",
    "the points in that window. If a line was detected before the window is slid around the margin of the previous polynomial line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractForFit(warped,LtLine, RtLine): \n",
    "    \n",
    "    histogram = getHist(warped)\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(warped.shape[0]/nwindows)\n",
    "    nonzero = warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])    \n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 30\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "  \n",
    "    if not LtLine.detected:\n",
    "        leftx_base = np.argmax(histogram[:midpoint])\n",
    "        leftx_current = leftx_base\n",
    "        # Step through the windows one by one\n",
    "        for window in range(nwindows):        \n",
    "            win_y_low = warped.shape[0] - (window+1)*window_height\n",
    "            win_y_high = warped.shape[0] - window*window_height\n",
    "            win_xleft_low = leftx_current - margin\n",
    "            win_xleft_high = leftx_current + margin\n",
    "            # Identify the nonzero pixels in x and y within the window\n",
    "            good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high)\\\n",
    "                              & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "\n",
    "            # Append these indices to the lists\n",
    "            left_lane_inds.append(good_left_inds)\n",
    "\n",
    "            # If you found > minpix pixels, recenter next window on their mean position\n",
    "            if len(good_left_inds) > minpix:\n",
    "                leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        # Concatenate the arrays of indices\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    \n",
    "    else:\n",
    "#         left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy+ left_fit[2] - margin)) & \\\n",
    "#                           (nonzerox < (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] + margin)))\n",
    "        left_lane_inds = ((nonzerox > (LtLine.best_fit[0]*(nonzeroy**2) + \\\n",
    "                                       LtLine.best_fit[1]*nonzeroy+ LtLine.best_fit[2] - margin)) & \\\n",
    "                  (nonzerox < (LtLine.best_fit[0]*(nonzeroy**2) + \\\n",
    "                               LtLine.best_fit[1]*nonzeroy + LtLine.best_fit[2] + margin)))\n",
    "\n",
    "        \n",
    "        \n",
    "    if not RtLine.detected:\n",
    "        rightx_base = np.argmax(histogram[midpoint:])+ midpoint\n",
    "        rightx_current = rightx_base\n",
    "        # Step through the windows one by one\n",
    "        for window in range(nwindows):        \n",
    "            win_y_low = warped.shape[0] - (window+1)*window_height\n",
    "            win_y_high = warped.shape[0] - window*window_height\n",
    "            win_xright_low = rightx_current - margin\n",
    "            win_xright_high = rightx_current + margin\n",
    "            \n",
    "            # Identify the nonzero pixels in x and y within the window\n",
    "            good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high)\\\n",
    "                              & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "\n",
    "            # Append these indices to the lists\n",
    "            right_lane_inds.append(good_right_inds)\n",
    "\n",
    "            # If you found > minpix pixels, recenter next window on their mean position\n",
    "            if len(good_right_inds) > minpix:\n",
    "                rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    else:\n",
    "        right_lane_inds = ((nonzerox > (RtLine.best_fit[0]*(nonzeroy**2) + RtLine.best_fit[1]*nonzeroy + RtLine.best_fit[2] - margin)) \\\n",
    "                           & (nonzerox < (RtLine.best_fit[0]*(nonzeroy**2) + RtLine.best_fit[1]*nonzeroy + RtLine.best_fit[2] + margin)))  \n",
    "\n",
    "\n",
    "\n",
    "            # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    \n",
    "     \n",
    "\n",
    "    \n",
    "    return leftx, lefty, rightx, righty           \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Line class helps store the appropriate data needed for the necessary radius and lane position calculations as well as information that will be used in the calcualtions of the line in the next frame. \n",
    "\n",
    "It was decided to store points over the course of several frames in order to get a better polynomial fit. This is especially helpful for the dotted lines and accross the bridges. Also the differences between the previous line and the current line are used to judge wether or not the current line is a valid estimate. Also the line_base_pos has been defined as the location where the line hits the bottom of the image. This is useful for determining the width of the lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Class containing characteristics of each line detection\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        # y values of the last n fits of the line\n",
    "        self.recent_yfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None \n",
    "        #previos frame best fit\n",
    "        self.prevbest_fit = None\n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None         \n",
    "        #Location where line hits bottom of image\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels in last n frames\n",
    "        self.arrayx = None  \n",
    "        #y values for detected line pixels in last n frames\n",
    "        self.array = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function take draws a shaded green region to mark the lane\n",
    "def markLane(image, warped, left_fit,right_fit):\n",
    "    # y points to mark line\n",
    "    ploty = np.linspace(0, image.shape[0]-1, num=image.shape[0])\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    color_warp = np.zeros_like(image).astype(np.uint8)\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (image.shape[1], image.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(image, 1, newwarp, 0.3, 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lane_pipeline funcion uses all the previously defined functions to calculat the lane locations. Two Line objects are created, one for the left and one for the right. The image is converted to the BGR opencV format, the lines are detected using the above described pipeline. The region of interest is masked and the image is transformed to a top down view. If a line is not detected the previous line is used. If a line is detected but is concidered invalid, again the previous line is used. Points from the last four frames are used to determine the polynomial fit of the line. If both lines are detected but the lane is judged to be too wide or too narrow the line that was closest to the previous line is used. Also if the difference in radius from the current detected line is greater than 3% of the difference in the radii from the previously detected lines only the line that changed the least is considered valid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculating the Radius of Curvature\n",
    "Once the lane points have been extracted and excess information removed a polyfit is run on the left and right lane pixels. The radius of curvature is determined using a formula described in this tutorial: http://www.intmath.com/applications-differentiation/8-radius-curvature.php. The key point is wether or not the polynomial is correctly calculated. If not enough information is in the pixels the wrong polynomial could be detected. This is why 4 frames of pixels was used in calculating the polynomials and thus the radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lane_pipelineV(image):\n",
    "    global loopn\n",
    "    global curLtLine\n",
    "    global curRtLine\n",
    "    global offCenter\n",
    "    \n",
    "        # undistor image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image = cv2.undistort(image, mtx, dist, None, mtx)\n",
    "    # detect prospective lines\n",
    "    combined = pipeline(image)\n",
    "    # mask out regions outside of expected lane region\n",
    "    masked = maskRegion(combined,vertices)  \n",
    "    # perfor merspective transform\n",
    "    warped = warp(masked, M)\n",
    "    \n",
    "    # extract left and right pixel location of perspective lane lines\n",
    "    leftx, lefty, rightx, righty = extractForFit(warped, curLtLine, curRtLine)\n",
    "    # get a 2nd order polynomial fit for prospective left and right lines\n",
    "    \n",
    "    \n",
    "    if len(leftx)>500:\n",
    "\n",
    "        curLtLine.current_fit = np.polyfit(lefty,leftx, 2)\n",
    "        curLtLine.detected = True\n",
    "    else:\n",
    "        curLtLine.detected = False  \n",
    "        \n",
    "    if len(rightx)>500:\n",
    "        curRtLine.current_fit = np.polyfit(righty,rightx, 2)\n",
    "        curRtLine.detected = True\n",
    "    else:\n",
    "        curRtLine.detected = False \n",
    "\n",
    "    \n",
    "    if curLtLine.detected:    \n",
    "        # keep last n frames of data for poly fit\n",
    "        if(len(curLtLine.recent_xfitted)<frames):\n",
    "            curLtLine.recent_xfitted.append(leftx)\n",
    "            curLtLine.recent_yfitted.append(lefty)\n",
    "        else:\n",
    "            curLtLine.recent_xfitted[loopn%frames] =leftx \n",
    "            curLtLine.recent_yfitted[loopn%frames] =lefty\n",
    "\n",
    "        curLtLine.arrayx = np.concatenate(curLtLine.recent_xfitted)\n",
    "        curLtLine.arrayy = np.concatenate(curLtLine.recent_yfitted)\n",
    "#         curLtLine.best_fit = np.polyfit(curLtLine.arrayy,\\\n",
    "#                                     curLtLine.arrayx, 2)\n",
    "        tempFitL = np.polyfit(curLtLine.arrayy,\\\n",
    "                                    curLtLine.arrayx, 2)\n",
    "\n",
    "        left_base = tempFitL[0]*y_eval**2 + tempFitL[1]*y_eval + tempFitL[2]\n",
    "        # curvature in meters\n",
    "        left_fit_m_temp = np.polyfit(curLtLine.arrayy*ym_per_pix, \\\n",
    "                                curLtLine.arrayx*xm_per_pix, 2)\n",
    "        temp_L_curve= ((1 + (2*left_fit_m_temp[0]*y_eval*ym_per_pix+left_fit_m_temp[1])**2)\\\n",
    "                       **1.5) /np.absolute(2*left_fit_m_temp[0])\n",
    "\n",
    "        \n",
    "    if curRtLine.detected:\n",
    "        # keep last n frames of data for poly fit\n",
    "        if(len(curRtLine.recent_xfitted)<frames):\n",
    "            curRtLine.recent_xfitted.append(rightx)\n",
    "            curRtLine.recent_yfitted.append(righty)\n",
    "        else:\n",
    "            curRtLine.recent_xfitted[loopn%frames] =rightx \n",
    "            curRtLine.recent_yfitted[loopn%frames] =righty\n",
    "            \n",
    "        curRtLine.arrayx = np.concatenate(curRtLine.recent_xfitted)\n",
    "        curRtLine.arrayy = np.concatenate(curRtLine.recent_yfitted)\n",
    "        tempFitR = np.polyfit(curRtLine.arrayy,\\\n",
    "                                        curRtLine.arrayx, 2)\n",
    "        right_base = tempFitR[0]*y_eval**2 + tempFitR[1]*y_eval + tempFitR[2] \n",
    "        \n",
    "        right_fit_m_temp = np.polyfit(curRtLine.arrayy*ym_per_pix, \\\n",
    "                                curRtLine.arrayx*xm_per_pix, 2)\n",
    "        \n",
    "        temp_R_curve= ((1 + (2*right_fit_m_temp[0]*y_eval*ym_per_pix+right_fit_m_temp[1])**2)\\\n",
    "                       **1.5) /np.absolute(2*right_fit_m_temp[0])\n",
    "\n",
    "\n",
    "    # Initial loop\n",
    "    if loopn is 0 or curRtLine.best_fit is None or curLtLine.best_fit is None:\n",
    "        if curLtLine.detected:\n",
    "            curLtLine.best_fit = tempFitL\n",
    "            curLtLine.line_base_pos = left_base\n",
    "            curLtLine.radius_of_curvature = temp_L_curve\n",
    "            left_fit_m = left_fit_m_temp\n",
    "        if curRtLine.detected:\n",
    "            curRtLine.best_fit = tempFitR\n",
    "            right_fit_m = right_fit_m_temp\n",
    "            curRtLine.radius_of_curvature = temp_R_curve\n",
    "            curRtLine.line_base_pos = right_base      \n",
    "    elif curRtLine.detected and curLtLine.detected and curLtLine.best_fit is not None and curRtLine.best_fit is not None:\n",
    "        laneWidthPx = abs(right_base - left_base)\n",
    "        # if lane to small or too big only update line that is closest to previous line\n",
    "        # or if the difference in radius changes by more than 10% of previous only update the closest line\n",
    "        if laneWidthPx*xm_per_pix < 3 or laneWidthPx*xm_per_pix > 4.4 or\\\n",
    "            abs(temp_R_curve - temp_L_curve) >1.03*abs(curLtLine.radius_of_curvature-curRtLine.radius_of_curvature):\n",
    "            leftChange = LA.norm(curLtLine.best_fit - curLtLine.current_fit)\n",
    "            rightChange = LA.norm(curRtLine.best_fit - curRtLine.current_fit)\n",
    "            if leftChange > rightChange :\n",
    "                curLtLine.detected = False # false set here so next frame recognizes previos line not good\n",
    "                curRtLine.best_fit = tempFitR\n",
    "                right_fit_m = right_fit_m_temp\n",
    "                curRtLine.radius_of_curvature = temp_R_curve\n",
    "                curRtLine.line_base_pos = right_base\n",
    "            else:\n",
    "                curRtLine.detected = False\n",
    "                curLtLine.best_fit = tempFitL\n",
    "                left_fit_m = left_fit_m_temp\n",
    "                curLtLine.radius_of_curvature = temp_L_curve\n",
    "                curLtLine.line_base_pos = left_base\n",
    "        else:\n",
    "            curRtLine.best_fit = tempFitR\n",
    "            right_fit_m = right_fit_m_temp        \n",
    "            curLtLine.best_fit = tempFitL\n",
    "            left_fit_m = left_fit_m_temp        \n",
    "            curRtLine.radius_of_curvature = temp_R_curve\n",
    "            curLtLine.radius_of_curvature = temp_L_curve\n",
    "            curRtLine.line_base_pos = right_base\n",
    "            curLtLine.line_base_pos = left_base\n",
    "    \n",
    "#     print(\" arraloc : \",loopn%frames)\n",
    "#     print(\"Left Detected :\", curLtLine.detected, \" Length lft_x : \", len(leftx))\n",
    "#     print(\"Right Detected :\", curRtLine.detected,\" Length Rt_x : \", len(rightx))\n",
    "\n",
    "    loopn = loopn+1\n",
    "    if curLtLine.best_fit is not None and curRtLine.best_fit is not None:\n",
    "        laneWidthPx = curRtLine.line_base_pos - curLtLine.line_base_pos\n",
    "        lane_center = laneWidthPx/2+curLtLine.line_base_pos\n",
    "        offCenter = (lane_center - imgmid)*xm_per_pix        \n",
    "        sign = np.sign(offCenter)\n",
    "\n",
    "        if sign>0:\n",
    "            side = \"Vehicle Right of Center by \"+str(offCenter)+\"m\"\n",
    "        else:\n",
    "            side = \"Vehicle Left of Center by \"+str(offCenter)+\"m\"\n",
    "        \n",
    "        if(curLtLine.best_fit[0] - curLtLine.line_base_pos)>0:\n",
    "            direct = \"Right\"\n",
    "        else:\n",
    "            direct = \"Left\"\n",
    "        radiusWords = \"Radius of curvature is \"+str(curLtLine.radius_of_curvature) + \"m to the \" + str(direct)\n",
    "        result = markLane(image, warped, curLtLine.best_fit, curRtLine.best_fit)#should use best fit\n",
    "        cv2.putText(result, side, (50,100), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,255,255),4)\n",
    "        cv2.putText(result, radiusWords, (50,200), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,255,255),4)  \n",
    "        \n",
    "        result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "        return result\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lane Detection Discussion\n",
    "\n",
    "The performance is quite acceptable for this particular highway. Difficulties could come in on different highways say where the road is concrete as opposed to asfault. The edge detection works best with the black background and ok on the bridges. Also any occlusions to the camera would potentially cause failures. Rain, snow, fog. Also changes in lighting could produce different performance. Also if the car were to get bumped by another driver and the lanes were to go out of the expected view the algorithm is not equiped to correct itself. More suffisticated contextual awareness would be needed for the less common scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Detection Pipeline\n",
    "![](test_images/cutout6.jpg) \n",
    "#### HOG Feature Detection\n",
    "Many different parameters were tried to get good HOG features from training images. Different colorspaces were examined to see which ones could best seperate vehicles from non vehicles. The first attempt used features from the r space of rgb, the s of hsv and the u from luv. This was able to produce 100% accuracy on the small training set. However, it was realized that this caused a five times increase in the time it took to calculate the features. Also, in the end when the large training set was used containing over 15000 images, the performance of the three color chanels of the YCrCb color space proved just as good and five times faster to calculate than the three seperate colorspaces. Also, it was tried to use 16 pixels and 1 cell per block. While this performed well on the training set it was not as good on the video data. Even though the number of features was doubled, in the end 8 pixels per cell was used with 2 cells per block. Also, both 9 and 6 orients were tried with 9 producing a slightly better performance. 9 gradient orientations was chosen in the end to get the highest accuracy on the video. \n",
    "\n",
    "The Hog features detection is located on cell 1 lines 20-38 in the python notebook. The parameters are set on lines 170-172.\n",
    " \n",
    "\n",
    "#### Train Classifier\n",
    "\n",
    "Both a linear SVM and Decision tree were tested in classifying vehicles or non vehicles. The features used for both classifiers were all three channels of the YCrCb color space for  spacial features, color histograms, and hog features. Using only histogram and spacial features the SVM was able to obtain a 91.7 % accuracy and the decision tree was able to achieve a 90.7 % accuracy on the large dataset. Adding the HOG features improved the accuracy to 99% using the SVM classifier. \n",
    "\n",
    "The spacial feature detector resized the image to 32 by 32 and unraveled the features. The hist features concatenated the histograms of each color chanel using 32 bins for each channel. The hog features were calculated using 9 orientations, 8 pixels per cell and 2 cells per block. \n",
    "\n",
    "The decision tree with default parameters achieved good performance around 97% but in the end the SVM with the linear classifier was able to get 99% accuracy on the large data set. The standard scaler was used to scale the features. \n",
    "\n",
    "#### Available Datasets\n",
    "\n",
    "Vehicle and NonVehicle Dataset taken from GTIvehicle image database\n",
    "\n",
    "http://www.gti.ssr.upm.es/data/Vehicle_database.html\n",
    "    \n",
    "as well as the KITTI vission benchmark suite\n",
    "\n",
    "http://www.cvlibs.net/datasets/kitti/\n",
    "\n",
    "and some images from the project video \n",
    "\n",
    "Vehicle Set: https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip\n",
    "Non-Vehicle Set: https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip\n",
    "        \n",
    "Small Vehicle Set: https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles_smallset.zip\n",
    "Small Non-Vehicle Set: https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles_smallset.zip\n",
    "\n",
    "#### Sliding Windows\n",
    "\n",
    "In order to capture vehicles of differing sizes defferent size search windows were used to classify cars or no cars. This parameter required much fine tuning on actual images as some smaller windows were prone to false detections and too large of windows werent useful. Using the area which a particular size car would be found generally determined the search reagion for a given size window. That is to say larger windows searched the bottom of the image while small windows searched closer to the middle. No images searched the upper half of the image. Many different sets of windows were tried to achieve best performance between detection and no false positives. \n",
    "\n",
    "#### Test Images and Pipeline Optimization\n",
    "\n",
    "The pipeline was optimized by examining which frames in the video failed and why. The heat maps created by positive detections where analyzed as well as the final result generated after thresholding. The threshold heatmap generally was increased for false positives. For lack of detection new search windows were created matchning the size of the vehicles missed. Also the hot boxes across two consecutive frames were used for detecting vehicles in the frame. This was to avoid nuances in a single frame which were filtered out by the threshold. \n",
    "\n",
    "#### Examples of Heat Map with Multiple Boxes\n",
    "\n",
    "Threshold too high for one car but captures other car\n",
    "![](test_images/windowimg1.png) \n",
    "Subsequent detection in next frame\n",
    "![](test_images/windowimg3.png) \n",
    "\n",
    "The theshold had to be low enough to capture all the cars while at the same time, high enough to block false positives.\n",
    "\n",
    "#### Method for filtering false positeves\n",
    "\n",
    "As discussed above two consecutive frames were used to detect vehicles. In this was a false positive would have to be detected in both frames to be classified. A heat map was used summing the number of boxes that detected a car over each area in an image. A threshold of 4 of 5 boxes was required to classify a vehicle. The label function was used to group all the pixels detected as a vehicle into one box containing the min and max x and y pixels to discribe the bounding box. \n",
    "\n",
    "Example: False detection filtered out\n",
    "![](test_images/windowimg22.png)\n",
    "\n",
    "### Discussion\n",
    "\n",
    "While the classifier performed well on the large training set, when using it to predict video images many more false positives were detected than expected. Also many vehicles were omitted than expected. This initial result made me realize the importance of the number of sliding windows, their size, and the amount of overlap they have as they are slid accross the screen. These parameters were critical to the success of the algorithm and had to be fine tuned over many iterations. Calculating the HOG features just once per image could speed up processing quite a bit however because the window sizes had to be tuned so heavily it is difficult to optimize them such that the same HOG detector can be used once for all sliding windows. \n",
    "\n",
    "\n",
    "While the box Sizes and search locations were tuned extensively as well as the heatmap threshold, there were a few images that failed filtering and produced the wrong result. These issues could probably be corrected with more training data.\n",
    "\n",
    "#### Examples of Failures\n",
    "\n",
    "False Positive\n",
    "![](test_images/windowimg145.png)\n",
    "\n",
    "Detection Failure: While the threshold prevents false positives it can also filter out correct detections. \n",
    "![](test_images/windowimg123.png)\n",
    "\n",
    "The pipeline is likely to fail on different types of roads with different features. Particularly on single lane roads where cars coming the other direction can be scene. Also city streets with lots of clutter would also be difficult for this algorithm. Also different lighting conditions will probably result in more failures. I think to make the algorithm more robust perhaps a nonlinear classifier could be used. Also, more training data would help. Rather than determine a set of features byu hand perhaps a convolutional neural network could be used to detect features with better performance. Algorithms like YOLO and SSD perform quite well on such tasks so looking into how these algorithms work could provide clues as to what techniques make more robust performance. It was nice to see the strengths and weaknesses of a very easy to implement SVM right out of the box and it is a good foundation to build upon in the future. \n",
    "\n",
    "https://pjreddie.com/darknet/yolo/\n",
    "\n",
    "https://github.com/weiliu89/caffe/tree/ssd\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
